{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxqKEXEUXzmN"
      },
      "outputs": [],
      "source": [
        "# S90G4LGOSBQDQINX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance alpha_vantage pandas\n",
        "# If you haven't installed transformers (for FinBERT)\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "GsT4XmaXZZl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Define stock ticker and time range\n",
        "ticker = \"AAPL\"\n",
        "start_date = \"2023-01-01\"\n",
        "end_date = \"2024-12-31\"\n",
        "\n",
        "# 2. Download data\n",
        "stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "# 3. Simple cleaning (keep only necessary columns)\n",
        "# yfinance columns are often MultiIndex, let's simplify\n",
        "stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "stock_data.reset_index(inplace=True) # Make 'Date' a regular column\n",
        "stock_data['Date'] = stock_data['Date'].dt.date # Keep only the date part, remove time\n",
        "\n",
        "print(f\"Successfully retrieved {len(stock_data)} days of market data\")\n",
        "stock_data.head()"
      ],
      "metadata": {
        "id": "rakiEaK3ZdXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"GOOGL_stock_data.csv\"\n",
        "\n",
        "# === Check: if file exists, read directly; otherwise, download ===\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Reading data from local CSV...\")\n",
        "    # index_col=0 means set the first column ('Date') as index, parse_dates=True automatically recognizes date format\n",
        "    stock_data = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    print(\"No local data, downloading from Yahoo Finance...\")\n",
        "    ticker = \"GOOGL\"\n",
        "    stock_data = yf.download(ticker, start=\"2023-01-01\", end=\"2024-12-31\")\n",
        "\n",
        "    # Simplify column structure (yfinance downloads sometimes have MultiIndex, making it cleaner)\n",
        "    stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "    # === Key step: save to CSV ===\n",
        "    stock_data.to_csv(file_path)\n",
        "    print(f\"Data saved to {file_path}\")\n",
        "\n",
        "# View first few rows\n",
        "print(stock_data.head())"
      ],
      "metadata": {
        "id": "FAvVCwLYbN9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Adding force_remount=True resolves the issue\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "fjiGdTSfOdrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import calendar # New: for accurately calculating days in each month\n",
        "from datetime import datetime\n",
        "\n",
        "# === Configuration Area ===\n",
        "API_KEY = \"need your api\"\n",
        "TICKER = \"AAPL\"\n",
        "SAVE_DIR = '/content/drive/MyDrive/StockData'\n",
        "FINAL_CSV_PATH = f\"{SAVE_DIR}/{TICKER}_2023_2024_Full_News.csv\"\n",
        "\n",
        "# Ensure save directory exists\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "def generate_monthly_periods(start_year, end_year):\n",
        "    \"\"\"Generates monthly start and end time strings from start_year to end_year (precisely fixed version)\"\"\"\n",
        "    periods = []\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in range(1, 13):\n",
        "            # Get the number of days in the month (calendar.monthrange returns (weekday, total_days))\n",
        "            _, last_day = calendar.monthrange(year, month)\n",
        "\n",
        "            start_date = f\"{year}{month:02d}01T0000\"\n",
        "            end_date = f\"{year}{month:02d}{last_day}T2359\"\n",
        "\n",
        "            periods.append((f\"{year}-{month:02d}\", start_date, end_date))\n",
        "    return periods\n",
        "\n",
        "def fetch_historical_news():\n",
        "    # 1. Generate all months for 2023-2024\n",
        "    all_periods = generate_monthly_periods(2023, 2024)\n",
        "\n",
        "    # 2. Check existing data for resume functionality\n",
        "    existing_months = []\n",
        "    all_news_data = []\n",
        "\n",
        "    if os.path.exists(FINAL_CSV_PATH):\n",
        "        print(f\"Existing file detected: {FINAL_CSV_PATH}, reading for resume...\")\n",
        "        try:\n",
        "            df_exist = pd.read_csv(FINAL_CSV_PATH)\n",
        "            if 'Date' in df_exist.columns:\n",
        "                # Convert to datetime objects for month extraction\n",
        "                df_exist['DateObj'] = pd.to_datetime(df_exist['Date'], errors='coerce')\n",
        "                # Extract existing months (format YYYY-MM)\n",
        "                existing_months = df_exist['DateObj'].dt.strftime('%Y-%m').unique().tolist()\n",
        "                # Restore data to list, ready to append\n",
        "                # Note: drop the temporary 'DateObj' column\n",
        "                all_news_data = df_exist.drop(columns=['DateObj']).to_dict('records')\n",
        "                print(f\"‚úÖ Detected {len(existing_months)} months of data (e.g.: {existing_months[:3]}).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read existing file, starting over: {e}\")\n",
        "\n",
        "    print(f\"Preparing to continue fetching news for {TICKER}...\")\n",
        "\n",
        "    requests_made = 0\n",
        "\n",
        "    for month_label, start_str, end_str in all_periods:\n",
        "        if month_label in existing_months:\n",
        "            continue # Skip already fetched months\n",
        "\n",
        "        print(f\"Requesting: {month_label} (Range: {start_str[:8]}-{end_str[:8]})...\", end=\" \")\n",
        "\n",
        "        url = (\n",
        "            f\"https://www.alphavantage.co/query?\"\n",
        "            f\"function=NEWS_SENTIMENT\"\n",
        "            f\"&tickers={TICKER}\"\n",
        "            f\"&time_from={start_str}\"\n",
        "            f\"&time_to={end_str}\"\n",
        "            f\"&limit=1000\"\n",
        "            f\"&apikey={API_KEY}\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "            requests_made += 1\n",
        "\n",
        "            # Error check\n",
        "            if \"Error Message\" in data:\n",
        "                print(f\"\\n‚ùå Parameter error: {data['Error Message']}\")\n",
        "                break\n",
        "            if \"Information\" in data or \"Note\" in data:\n",
        "                print(f\"\\n‚ö†Ô∏è API limit warning: {data.get('Information', data.get('Note'))}\")\n",
        "                print(\"Stopping fetch. Please save current progress.\")\n",
        "                break\n",
        "\n",
        "            if \"feed\" in data:\n",
        "                items = data[\"feed\"]\n",
        "                print(f\"Success, retrieved {len(items)} items.\")\n",
        "\n",
        "                for item in items:\n",
        "                    raw_time = item.get('time_published', '')\n",
        "                    date_val = f\"{raw_time[:4]}-{raw_time[4:6]}-{raw_time[6:8]}\" if len(raw_time) >= 8 else \"Unknown\"\n",
        "\n",
        "                    all_news_data.append({\n",
        "                        \"Date\": date_val,\n",
        "                        \"Title\": item.get('title', ''),\n",
        "                        \"Summary\": item.get('summary', ''),\n",
        "                        \"Source\": item.get('source', ''),\n",
        "                        \"URL\": item.get('url', ''),\n",
        "                        \"Sentiment_Score\": item.get('overall_sentiment_score', 0),\n",
        "                        \"Sentiment_Label\": item.get('overall_sentiment_label', 'Neutral')\n",
        "                    })\n",
        "            else:\n",
        "                # May be an empty month, or quota exhausted without standard error\n",
        "                print(f\"No data or abnormal response (Keys: {list(data.keys())})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Network request error: {e}\")\n",
        "\n",
        "        # Save immediately after each successful request\n",
        "        if all_news_data:\n",
        "            df_temp = pd.DataFrame(all_news_data)\n",
        "            # Simple deduplication\n",
        "            df_temp.drop_duplicates(subset=['Title', 'Date'], inplace=True)\n",
        "            df_temp.sort_values(by='Date', inplace=True)\n",
        "            df_temp.to_csv(FINAL_CSV_PATH, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        # Rate control: sleep 15 seconds (safer)\n",
        "        time.sleep(15)\n",
        "\n",
        "    # Final results\n",
        "    if all_news_data:\n",
        "        df_final = pd.DataFrame(all_news_data)\n",
        "        df_final.drop_duplicates(subset=['Title', 'Date'], inplace=True)\n",
        "        print(f\"\\nüéâ Task finished! Collected a total of {len(df_final)} news items.\")\n",
        "        print(f\"File location: {FINAL_CSV_PATH}\")\n",
        "    else:\n",
        "        print(\"\\nFailed to retrieve any data.\")\n",
        "\n",
        "fetch_historical_news()"
      ],
      "metadata": {
        "id": "vjgwqZsqOYT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install finnhub-python gnews"
      ],
      "metadata": {
        "id": "OyF3giYaY09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import finnhub\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# === Enter your Finnhub API Key here ===\n",
        "FINNHUB_API_KEY = \"need your api\"\n",
        "# ================================\n",
        "\n",
        "finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)\n",
        "TICKER = \"AAPL\"\n",
        "SAVE_DIR = '/content/drive/MyDrive/StockData'\n",
        "CSV_PATH = f\"{SAVE_DIR}/{TICKER}_Finnhub_2023_2024.csv\"\n",
        "\n",
        "def fetch_finnhub_news():\n",
        "    print(\"Starting data retrieval from Finnhub...\")\n",
        "\n",
        "    # Generate monthly splits from 2023-01-01 to 2024-12-31\n",
        "    # Finnhub format requirement: YYYY-MM-DD\n",
        "    periods = []\n",
        "    for year in [2023, 2024]:\n",
        "        for month in range(1, 13):\n",
        "            # Simple handling for the last day of each month\n",
        "            if month in [1,3,5,7,8,10,12]: d=31\n",
        "            elif month==2: d=29 # Sufficient to cover leap years\n",
        "            else: d=30\n",
        "            periods.append((f\"{year}-{month:02d}-01\", f\"{year}-{month:02d}-{d}\"))\n",
        "\n",
        "    all_news = []\n",
        "\n",
        "    for start_date, end_date in periods:\n",
        "        print(f\"Fetching: {start_date} to {end_date} ...\", end=\" \")\n",
        "\n",
        "        try:\n",
        "            # Finnhub API call\n",
        "            res = finnhub_client.company_news(TICKER, _from=start_date, to=end_date)\n",
        "\n",
        "            if len(res) > 0:\n",
        "                print(f\"‚úÖ Retrieved {len(res)} items\")\n",
        "                for item in res:\n",
        "                    # Finnhub returns Unix timestamp, needs conversion\n",
        "                    ts = int(item.get('datetime', 0))\n",
        "                    date_str = datetime.fromtimestamp(ts).strftime('%Y-%m-%d')\n",
        "\n",
        "                    all_news.append({\n",
        "                        \"Date\": date_str,\n",
        "                        \"Title\": item.get('headline'),\n",
        "                        \"Summary\": item.get('summary'),\n",
        "                        \"Source\": item.get('source'),\n",
        "                        \"URL\": item.get('url')\n",
        "                    })\n",
        "            else:\n",
        "                print(\"‚ùå No data (possibly due to free tier historical range limitation)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        # Finnhub limit is 60 requests per second (very generous), but for safety we pause for 1 second\n",
        "        time.sleep(1)\n",
        "\n",
        "    if all_news:\n",
        "        df = pd.DataFrame(all_news)\n",
        "        df.drop_duplicates(subset=['Title', 'Date'], inplace=True)\n",
        "        df.sort_values(by='Date', inplace=True)\n",
        "        df.to_csv(CSV_PATH, index=False, encoding='utf-8-sig')\n",
        "        print(f\"\\nüéâ Finnhub fetch complete! Total {len(df)} items. Saved to: {CSV_PATH}\")\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"\\nFailed to retrieve data. Please check if Key is correct or free tier limitations.\")\n",
        "\n",
        "fetch_finnhub_news()"
      ],
      "metadata": {
        "id": "UBpbE2vqY9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gnews --quiet\n",
        "\n",
        "import pandas as pd\n",
        "from gnews import GNews\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import calendar\n",
        "from datetime import datetime\n",
        "\n",
        "# === Configuration ===\n",
        "KEYWORD = \"Apple\"  # Keep broad, filter by code\n",
        "SAVE_DIR = '/content/drive/MyDrive/StockData'\n",
        "CSV_PATH = f\"{SAVE_DIR}/AAPL_GNews_2023_2024_Cleaned.csv\"\n",
        "\n",
        "# === üçé Anti-fruit/Anti-noise Blacklist ===\n",
        "# If title or summary contains these words, discard directly\n",
        "NOISE_BLACKLIST = [\n",
        "    'pie', 'tart', 'recipe', 'sauce', 'cider', 'vinegar', # Food\n",
        "    'fruit', 'harvest', 'orchard', 'farm', 'agriculture', # Agriculture\n",
        "    'juice', 'smoothie', 'nutrition', 'diet', 'baking',   # Diet\n",
        "    'fiona apple', 'apple martin' # Celebrities (singer/star children)\n",
        "]\n",
        "\n",
        "# === Ensure directory exists ===\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "def is_relevant(text):\n",
        "    \"\"\"Checks if the text contains words from the blacklist\"\"\"\n",
        "    if not text:\n",
        "        return True\n",
        "    text_lower = text.lower()\n",
        "    for noise_word in NOISE_BLACKLIST:\n",
        "        # Adding spaces is to prevent false positives (e.g., 'grapefruit' contains 'fruit', but only 'fruit' as a standalone word should be filtered)\n",
        "        # Simple handling here, directly matching substring\n",
        "        if f\" {noise_word} \" in f\" {text_lower} \":\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def fetch_gnews_smart():\n",
        "    print(f\"üöÄ Starting smart fetch: 2023-2024 Apple News\")\n",
        "    print(f\"üõ°Ô∏è 'Anti-fruit' filter enabled: automatically discarding words like {NOISE_BLACKLIST[:3]}... etc.\")\n",
        "\n",
        "    google_news = GNews(language='en', country='US', max_results=100)\n",
        "\n",
        "    all_news = []\n",
        "    total_months = 24\n",
        "    processed_months = 0\n",
        "\n",
        "    for year in [2023, 2024]:\n",
        "        for month in range(1, 13):\n",
        "\n",
        "            _, last_day = calendar.monthrange(year, month)\n",
        "            start_date = (year, month, 1)\n",
        "            end_date = (year, month, last_day)\n",
        "\n",
        "            print(f\"[{processed_months+1}/{total_months}] üîç Searching: {year}-{month:02d} ...\", end=\" \")\n",
        "\n",
        "            try:\n",
        "                google_news.start_date = start_date\n",
        "                google_news.end_date = end_date\n",
        "\n",
        "                news_chunk = google_news.get_news(KEYWORD)\n",
        "\n",
        "                if news_chunk:\n",
        "                    count_before = len(news_chunk)\n",
        "                    valid_items = []\n",
        "\n",
        "                    for item in news_chunk:\n",
        "                        title = item.get('title', '')\n",
        "                        summary = item.get('description', '')\n",
        "\n",
        "                        # === Core filtering logic ===\n",
        "                        # Only keep if neither title nor summary contains \"fruit words\"\n",
        "                        if is_relevant(title) and is_relevant(summary):\n",
        "                            # Format alignment\n",
        "                            raw_date = item.get('published date', '')\n",
        "                            valid_items.append({\n",
        "                                \"Date\": raw_date,\n",
        "                                \"Title\": title,\n",
        "                                \"Summary\": summary,\n",
        "                                \"Source\": item.get('publisher', {}).get('title', 'Google News'),\n",
        "                                \"URL\": item.get('url', '')\n",
        "                            })\n",
        "\n",
        "                    count_after = len(valid_items)\n",
        "                    filtered_count = count_before - count_after\n",
        "\n",
        "                    if filtered_count > 0:\n",
        "                        print(f\"‚úÖ Fetched {count_before} items (discarded {filtered_count} irrelevant news)\")\n",
        "                    else:\n",
        "                        print(f\"‚úÖ Fetched {count_after} items\")\n",
        "\n",
        "                    all_news.extend(valid_items)\n",
        "\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è No results\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "            processed_months += 1\n",
        "            # Random sleep\n",
        "            time.sleep(random.uniform(5, 8))\n",
        "\n",
        "    # === Save ===\n",
        "    if all_news:\n",
        "        df = pd.DataFrame(all_news)\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "        df.sort_values(by='Date', inplace=True)\n",
        "        df.drop_duplicates(subset=['Title'], inplace=True)\n",
        "\n",
        "        df.to_csv(CSV_PATH, index=False, encoding='utf-8-sig')\n",
        "        print(f\"\\nüéâ Complete! Fetched {len(df)} high-quality data items.\")\n",
        "        print(f\"File location: {CSV_PATH}\")\n",
        "    else:\n",
        "        print(\"\\nüò≠ No data fetched.\")\n",
        "\n",
        "fetch_gnews_smart()"
      ],
      "metadata": {
        "id": "qmGxbT_1azu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gnews --quiet\n",
        "\n",
        "import pandas as pd\n",
        "from gnews import GNews\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import calendar\n",
        "from datetime import datetime\n",
        "\n",
        "# === üìÅ Save Path ===\n",
        "SAVE_DIR = '/content/drive/MyDrive/StockData'\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "# === üß† Core Configuration: Stock-specific Keywords and Blacklists ===\n",
        "# Format: Ticker: { 'keyword': search_term, 'blacklist': [exclusion_terms] }\n",
        "TICKER_CONFIG = {\n",
        "    \"AAPL\": {\n",
        "        \"keyword\": \"Apple\",\n",
        "        \"blacklist\": [\n",
        "            'pie', 'tart', 'recipe', 'sauce', 'cider', 'vinegar', 'juice',\n",
        "            'fruit', 'harvest', 'orchard', 'farm', 'nutrition', 'diet',\n",
        "            'fiona apple', 'bake', 'cake'\n",
        "        ]\n",
        "    },\n",
        "    \"MSFT\": {\n",
        "        \"keyword\": \"Microsoft\",\n",
        "        \"blacklist\": [\n",
        "            # Operations/Beginner Tutorials\n",
        "            'how to fix', 'error code', 'blue screen', 'bsod', 'kb50', 'update fail',\n",
        "            'download', 'install', 'wallpaper', 'shortcut', 'regedit',\n",
        "            # Pure Gaming/Entertainment\n",
        "            'walkthrough', 'guide', 'boss fight', 'controller skin', 'giveaway',\n",
        "            'deal alert', 'price drop'\n",
        "        ]\n",
        "    },\n",
        "    \"AMZN\": {\n",
        "        \"keyword\": \"Amazon\",\n",
        "        \"blacklist\": [\n",
        "            # Geography/Environment\n",
        "            'rainforest', 'jungle', 'deforestation', 'river', 'brazil', 'tribe',\n",
        "            'wildfire', 'indigenous', 'carbon sink',\n",
        "            # Pure Shopping/Marketing\n",
        "            'gift guide', 'best deal', 'coupon', 'discount', 'dupe',\n",
        "            'bestseller', 'fashion find'\n",
        "        ]\n",
        "    },\n",
        "    \"GOOGL\": {\n",
        "        \"keyword\": \"Google\",\n",
        "        \"blacklist\": [\n",
        "            # Miscellaneous\n",
        "            'doodle', 'easter egg', 'funny', 'meme', 'song',\n",
        "            # Basic Education\n",
        "            'phonics', 'toddler', 'alphabet song', 'preschool', 'soup', 'noodle'\n",
        "        ]\n",
        "    },\n",
        "    \"META\": {\n",
        "        \"keyword\": \"Meta\",\n",
        "        \"blacklist\": [\n",
        "            # Academic/Medical (Meta-analysis is the biggest noise source)\n",
        "            'meta-analysis', 'systematic review', 'clinical trial', 'genome',\n",
        "            'metabolism', 'metaphysics', 'poetry', 'fiction', 'rpg',\n",
        "            'tier list', 'loadout'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "def is_relevant(text, blacklist):\n",
        "    \"\"\"General filter: checks if the text contains blacklisted words for the stock\"\"\"\n",
        "    if not text:\n",
        "        return True\n",
        "    text_lower = text.lower()\n",
        "    for noise_word in blacklist:\n",
        "        # Match with spaces before and after to prevent false positives\n",
        "        # Simple handling here, directly matching substring\n",
        "        if f\" {noise_word} \" in f\" {text_lower} \":\n",
        "            return False\n",
        "        # Special handling for hyphenated words like meta-analysis\n",
        "        if noise_word in text_lower:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def fetch_ticker_news(ticker):\n",
        "    config = TICKER_CONFIG[ticker]\n",
        "    keyword = config['keyword']\n",
        "    blacklist = config['blacklist']\n",
        "\n",
        "    csv_filename = f\"{SAVE_DIR}/{ticker}_GNews_2023_2024.csv\"\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üíº Processing stock: {ticker} (Search term: {keyword})\")\n",
        "    print(f\"üö´ Blacklist loaded ({len(blacklist)} items): {blacklist[:3]}...\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    google_news = GNews(language='en', country='US', max_results=100)\n",
        "\n",
        "    all_news = []\n",
        "    total_months = 24\n",
        "    processed_months = 0\n",
        "\n",
        "    # Iterate over years\n",
        "    for year in [2023, 2024]:\n",
        "        for month in range(1, 13):\n",
        "\n",
        "            _, last_day = calendar.monthrange(year, month)\n",
        "            start_date = (year, month, 1)\n",
        "            end_date = (year, month, last_day)\n",
        "\n",
        "            print(f\"[{ticker}][{processed_months+1}/{total_months}] üîç {year}-{month:02d} ...\", end=\" \")\n",
        "\n",
        "            try:\n",
        "                google_news.start_date = start_date\n",
        "                google_news.end_date = end_date\n",
        "\n",
        "                news_chunk = google_news.get_news(keyword)\n",
        "\n",
        "                if news_chunk:\n",
        "                    valid_items = []\n",
        "                    for item in news_chunk:\n",
        "                        title = item.get('title', '')\n",
        "                        summary = item.get('description', '')\n",
        "\n",
        "                        # === Filtering using stock-specific blacklist ===\n",
        "                        if is_relevant(title, blacklist) and is_relevant(summary, blacklist):\n",
        "                            valid_items.append({\n",
        "                                \"Date\": item.get('published date', ''),\n",
        "                                \"Title\": title,\n",
        "                                \"Summary\": summary,\n",
        "                                \"Source\": item.get('publisher', {}).get('title', 'Google News'),\n",
        "                                \"URL\": item.get('url', '')\n",
        "                            })\n",
        "\n",
        "                    print(f\"‚úÖ Fetched {len(valid_items)} items (original {len(news_chunk)} items)\")\n",
        "                    all_news.extend(valid_items)\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è No data\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "            processed_months += 1\n",
        "            # Small sleep between months\n",
        "            time.sleep(random.uniform(3, 6))\n",
        "\n",
        "    # === Save data for this stock ===\n",
        "    if all_news:\n",
        "        df = pd.DataFrame(all_news)\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "        df.sort_values(by='Date', inplace=True)\n",
        "        df.drop_duplicates(subset=['Title'], inplace=True)\n",
        "\n",
        "        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"\\nüéâ {ticker} Complete! Total {len(df)} items. Saved to: {csv_filename}\")\n",
        "    else:\n",
        "        print(f\"\\nüò≠ {ticker} No data fetched.\")\n",
        "\n",
        "# === üî• Main Loop: Process 5 stocks sequentially ===\n",
        "target_tickers = [\"AAPL\", \"AMZN\", \"GOOGL\", \"META\", \"MSFT\"]\n",
        "\n",
        "for i, t in enumerate(target_tickers):\n",
        "    fetch_ticker_news(t)\n",
        "\n",
        "    # === Key: Long sleep between stocks ===\n",
        "    # To prevent Google from detecting continuous high-intensity scraping from the same IP\n",
        "    if i < len(target_tickers) - 1:\n",
        "        sleep_time = random.uniform(30, 60)\n",
        "        print(f\"\\n‚òï Taking a break... Pausing for {int(sleep_time)} seconds to prevent IP blocking...\\n\")\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "print(\"\\nüèÜ All tasks completed!\")"
      ],
      "metadata": {
        "id": "SU69zip5EbF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}